from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
import json
import asyncio
from typing import List, Dict, Optional
from dotenv import load_dotenv

load_dotenv()

# Load procedure definitions
with open("/home/namnp/ChatBotSynthetic/prompts/procedure.json", "r", encoding="utf-8") as f:
    procedure = json.load(f)
with open("/home/namnp/ChatBotSynthetic/data/conversation_without_image.json", "r", encoding="utf-8") as f:
    conversations = json.load(f)[:1]


class ThinkingTeacherResponse(BaseModel):
    """Response model for Thinking Teacher"""
    reasoning: str = Field(..., description="Ph√¢n t√≠ch ng·∫Øn g·ªçn: B∆∞·ªõc hi·ªán t·∫°i trong procedure, t√¨nh hu·ªëng KH, h√†nh ƒë·ªông c·∫ßn th·ª±c hi·ªán")
    corrected_response: Optional[str] = Field(None, description="C√¢u tr·∫£ l·ªùi ƒë√£ ƒë∆∞·ª£c s·ª≠a theo procedure (n·∫øu c·∫ßn s·ª≠a). ƒê·ªÉ null n·∫øu response g·ªëc ƒë√£ ƒë√∫ng")
    compliance_check: str = Field(..., description="OK n·∫øu tu√¢n th·ªß procedure, ho·∫∑c m√¥ t·∫£ ng·∫Øn g·ªçn v·∫•n ƒë·ªÅ n·∫øu kh√¥ng tu√¢n th·ªß")
    

class ThinkingTeacher:
    """Agent that thinks step-by-step before answering"""

    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.2):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature
        ).with_structured_output(ThinkingTeacherResponse)

    def _build_system_prompt(self, procedure_detail: str, category: str, sub_category: str, intentions: str) -> str:
        """Build system prompt for the Thinking Teacher"""
        prompt = f"""B·∫°n l√† m·ªôt nh√¢n vi√™n CSKH Heineken Vietnam gi√†u kinh nghi·ªám, ƒë√£ thu·ªôc n·∫±m l√≤ng quy tr√¨nh x·ª≠ l√Ω.

NG·ªÆ C·∫¢NH:
- Category: {category}
- Sub-Category: {sub_category}
- Intentions: {intentions}

QUY TR√åNH B·∫†N ƒê√É N·∫ÆM R√ï:
{procedure_detail}

VAI TR√í C·ª¶A B·∫†N:
B·∫°n ƒëang review l·∫°i c√°ch m·ªôt ƒë·ªìng nghi·ªáp m·ªõi x·ª≠ l√Ω case. V·ªõi m·ªói c√¢u tr·∫£ l·ªùi c·ªßa h·ªç, b·∫°n s·∫Ω:
1. Nghƒ© trong ƒë·∫ßu v·ªÅ t√¨nh hu·ªëng (nh∆∞ m·ªôt nh√¢n vi√™n th·∫≠t ƒëang ƒë·ªçc tin nh·∫Øn KH)
2. ƒê√°nh gi√° xem ƒë·ªìng nghi·ªáp c√≥ x·ª≠ l√Ω ƒë√∫ng kh√¥ng
3. N·∫øu sai, s·ª≠a l·∫°i cho h·ªç theo c√°ch b·∫°n s·∫Ω tr·∫£ l·ªùi

C·∫¶U THINKING (S·ª®Y NGHƒ® N·ªòI T√ÇM):
Vi·∫øt nh∆∞ th·ªÉ b·∫°n ƒëang t·ª± n√≥i chuy·ªán v·ªõi ch√≠nh m√¨nh khi ƒë·ªçc tin nh·∫Øn:
- T·ª± nhi√™n, ng·∫Øn g·ªçn, nh∆∞ suy nghƒ© th·∫≠t
- Kh√¥ng c·∫ßn n√≥i "B∆∞·ªõc 1, B∆∞·ªõc 2..." m·ªôt c√°ch c·ª©ng nh·∫Øc
- T·∫≠p trung v√†o: "KH ƒëang c·∫ßn g√¨?" ‚Üí "M√¨nh c·∫ßn l√†m g√¨?" ‚Üí "C√≥ v·∫•n ƒë·ªÅ g√¨ kh√¥ng?"

V√ç D·ª§ THINKING T·ªêT (t·ª± nhi√™n nh∆∞ ng∆∞·ªùi):
‚úÖ "KH h·ªèi m√£ NV ƒë·ªÉ c√†i app nh∆∞ng th·ª±c ra ƒë√£ c√≥ t√†i kho·∫£n r·ªìi. C·∫ßn h·ªèi th√¥ng tin ƒë·ªÉ tra c·ª©u xem ƒë√£ ƒëƒÉng k√Ω ch∆∞a, tr√°nh t·∫°o t√†i kho·∫£n tr√πng."
‚úÖ "OK, KH qu√™n m·∫≠t kh·∫©u. C·∫ßn h∆∞·ªõng d·∫´n Qu√™n MK ƒë·∫ßy ƒë·ªß: nh·∫≠p m√£ ‚Üí OTP ‚Üí t·∫°o MK m·ªõi 12 k√Ω t·ª±. ƒê·ªìng nghi·ªáp thi·∫øu m·∫•t b∆∞·ªõc OTP."
‚úÖ "ƒê∆°n gi·∫£n, KH c·∫£m ∆°n r·ªìi. Ch·ªâ c·∫ßn h·ªèi th√™m c√≥ c·∫ßn g√¨ kh√¥ng, nh·∫Øc hotline, xong."

V√ç D·ª§ THINKING T·ªÜ (c·ª©ng nh·∫Øc, h·ªçc thu·ªôc):
‚ùå "B∆∞·ªõc 1: Nh·∫≠n di·ªán nhu c·∫ßu v√† x√°c th·ª±c s∆° b·ªô. Agent ch∆∞a x√°c ƒë·ªãnh r√µ nhu c·∫ßu KH v√† kh√¥ng h∆∞·ªõng d·∫´n ƒë√∫ng quy tr√¨nh ƒë·ªïi m·∫≠t kh·∫©u."
‚ùå "B∆∞·ªõc 3: KH qu√™n m·∫≠t kh·∫©u. Agent ch∆∞a h∆∞·ªõng d·∫´n chi ti·∫øt c√°ch ƒë·∫∑t l·∫°i m·∫≠t kh·∫©u qua 'Qu√™n m·∫≠t kh·∫©u'."
‚ùå "Tr∆∞·ªõc ti√™n c·∫ßn ph√¢n t√≠ch t√¨nh hu·ªëng kh√°ch h√†ng ƒëang g·∫∑p ph·∫£i..."

Y√äU C·∫¶U KHI S·ª¨A C√ÇU TR·∫¢ L·ªúI:
- Gi·ªØ vƒÉn phong th√¢n thi·ªán, x∆∞ng h√¥ t·ª± nhi√™n (anh/ch·ªã/em)
- Ng·∫Øn g·ªçn, ƒë·ªß √Ω, kh√¥ng r∆∞·ªùm r√†
- Nh∆∞ c√°ch b·∫°n s·∫Ω chat th·∫≠t v·ªõi KH

COMPLIANCE CHECK:
- N·∫øu OK ‚Üí ch·ªâ vi·∫øt "OK"
- N·∫øu c√≥ v·∫•n ƒë·ªÅ ‚Üí vi·∫øt ng·∫Øn g·ªçn v·∫•n ƒë·ªÅ g√¨ (VD: "Thi·∫øu b∆∞·ªõc OTP", "Ch∆∞a h·ªèi th√¥ng tin ƒë·ªãnh danh")
"""
        return prompt
    async def process_conversation(self, conversation: Dict) -> Dict:
        """Process entire conversation with memory of previous turns"""
        # Get procedure and metadata
        procedure_id = str(conversation.get('procedure', '2'))  # Default to procedure 2 (forget password)
        procedure_detail = procedure.get(procedure_id, {}).get('detail_description', 'N/A')

        category = conversation.get('Category', 'N/A')
        sub_category = conversation.get('Sub_Category', 'N/A')
        intentions = conversation.get('Intentions', 'N/A')

        # Build system prompt once
        system_prompt = self._build_system_prompt(procedure_detail, category, sub_category, intentions)

        # Initialize conversation memory
        memory = [SystemMessage(content=system_prompt)]

        original_messages = conversation.get('messages', [])
        enhanced_messages = []

        # Process each turn
        i = 0
        while i < len(original_messages):
            current_msg = original_messages[i]

            # User message - just add to memory and output
            if current_msg.get('role') == 'user':
                enhanced_messages.append(current_msg)
                i += 1
                continue

            # Assistant message - need to evaluate and enhance
            if current_msg.get('role') == 'assistant':
                # Get corresponding user message (should be previous message)
                user_msg = original_messages[i-1] if i > 0 else {'role': 'user', 'content': ''}

                # Format current turn for evaluation
                current_turn = f"""L·ªäCH S·ª¨ H·ªòI THO·∫†I TR∆Ø·ªöC ƒê√ì:
{self._format_memory_history(enhanced_messages)}

TURN HI·ªÜN T·∫†I C·∫¶N ƒê√ÅNH GI√Å:
KH: {user_msg.get('content', '')}
Agent (g·ªëc): {current_msg.get('content', '')}

H√£y ƒë√°nh gi√° v√† s·ª≠a c√¢u tr·∫£ l·ªùi agent n·∫øu c·∫ßn."""

                # Add to memory and get response
                memory.append(HumanMessage(content=current_turn))
                response: ThinkingTeacherResponse = await self.llm.ainvoke(memory)

                # Determine final response (corrected or original)
                final_response = response.corrected_response if response.corrected_response else current_msg.get('content', '')

                # Create enhanced message with thinking
                enhanced_msg = {
                    'role': 'assistant',
                    'content': final_response,
                    'thinking': response.reasoning,
                    'compliance': response.compliance_check,
                    'original_content': current_msg.get('content', '') if response.corrected_response else None
                }
                enhanced_messages.append(enhanced_msg)

                # Update memory with the corrected version for next turn
                memory.append(AIMessage(content=f"[Thinking: {response.reasoning}]\n{final_response}"))

                i += 1

        # Return enhanced conversation
        conversation['messages'] = enhanced_messages
        return conversation

    def _format_memory_history(self, messages: List[Dict]) -> str:
        """Format previous messages for context"""
        if not messages:
            return "(Ch∆∞a c√≥ l·ªãch s·ª≠)"

        history_lines = []
        for msg in messages:
            role = msg.get('role')
            content = msg.get('content', '')
            if role == 'user':
                history_lines.append(f"KH: {content}")
            elif role == 'assistant':
                # Show the final response (could be corrected or original)
                history_lines.append(f"Agent: {content}")

        return "\n".join(history_lines)
    


async def main():
    """Main function with semaphore for concurrent processing"""
    # Semaphore to limit concurrent API calls
    semaphore = asyncio.Semaphore(5)  # Adjust this number based on your API rate limits

    async def process_with_semaphore(conv: Dict, index: int) -> Dict:
        """Process conversation with semaphore control"""
        async with semaphore:
            print(f"Processing conversation {index + 1}/{len(conversations)}...")
            teacher = ThinkingTeacher()
            try:
                result = await teacher.process_conversation(conv)
                print(f"‚úì Completed conversation {index + 1}")
                return result
            except Exception as e:
                print(f"‚úó Error in conversation {index + 1}: {str(e)}")
                # Return original conversation if error occurs
                return conv

    # Process all conversations concurrently with semaphore
    tasks = [process_with_semaphore(conv, i) for i, conv in enumerate(conversations)]
    thinking_conversations = await asyncio.gather(*tasks)

    # Save results
    output_path = "/home/namnp/ChatBotSynthetic/data/thinking_teacher_conversations.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(thinking_conversations, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Successfully processed {len(thinking_conversations)} conversations")
    print(f"üìù Saved to: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())