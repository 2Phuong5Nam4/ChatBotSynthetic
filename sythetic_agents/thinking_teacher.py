from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
import json
import asyncio
from typing import List, Dict, Optional
from dotenv import load_dotenv

load_dotenv()

# Load procedure definitions
with open("/home/namnp/ChatBotSynthetic/data/procedure.json", "r", encoding="utf-8") as f:
    procedure = json.load(f)
with open("/home/namnp/ChatBotSynthetic/data/synthetic_conversation.json", "r", encoding="utf-8") as f:
    conversations = json.load(f)
    # # load 5 conversaiont with 5 different procedure_id for testing
    # current_ids = set()
    # filtered_conversations = []
    # for conv in conversations:
    #     pid = str(conv.get('procedure_id'))
    #     if pid not in current_ids and pid in procedure:
    #         filtered_conversations.append(conv)
    #         current_ids.add(pid)
    #     if len(current_ids) >= 5:
    #         break
    # conversations = filtered_conversations

class ThinkingTeacherResponse(BaseModel):
    """Response model for Thinking Teacher"""
    reasoning: str = Field(..., description="Ph√¢n t√≠ch ng·∫Øn g·ªçn: B∆∞·ªõc hi·ªán t·∫°i trong procedure, t√¨nh hu·ªëng KH, h√†nh ƒë·ªông c·∫ßn th·ª±c hi·ªán")
    

class ThinkingTeacher:
    """Agent that thinks step-by-step before answering"""

    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.2):
        self.model_name = model_name
        self.temperature = temperature

    def _build_system_prompt(self, procedure_name: str, procedure_detail: str) -> str:
        """Build system prompt for the Thinking Teacher"""
        prompt = f"""B·∫°n l√† nh√¢n vi√™n CSKH Heineken Vietnam, ƒëang x·ª≠ l√Ω case cho kh√°ch h√†ng.

## QUY TR√åNH B·∫†N ƒê√É N·∫ÆM R√ï:
T√™n quy tr√¨nh: 
{procedure_name}
chi ti·∫øt quy tr√¨nh:
{procedure_detail}

## NHI·ªÜM V·ª§:
Vi·∫øt thinking - k·∫ø ho·∫°ch h√†nh ƒë·ªông ng·∫Øn g·ªçn TR∆Ø·ªöC KHI tr·∫£ l·ªùi KH. Thinking n√†y c·∫ßn tu√¢n th·ªß theo quy trinh ƒë√£ cho ƒë·ªÉ ƒë·∫£m b·∫£o x·ª≠ l√Ω ƒë√∫ng v√† nhanh ch√≥ng.

FORMAT THINKING:
Nh·∫≠n di·ªán t√¨nh hu·ªëng: ...
X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: ...
X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i trong quy tr√¨nh: ...
X√°c ƒë·ªãnh chu·ªói h√†nh ƒë·ªông ti·∫øp theo: action A ‚Üí action B ‚Üí action C ...

D√πng m≈©i t√™n (‚Üí) ƒë·ªÉ chain c√°c h√†nh ƒë·ªông, nh√≥m chi ti·∫øt v√†o ngo·∫∑c ƒë∆°n ()

V√ç D·ª§ THINKING T·ªêT (ng·∫Øn g·ªçn, theo ƒë√∫ng format):
‚úÖ
Nh·∫≠n di·ªán t√¨nh hu·ªëng: KH ch·ªß ƒëi·ªÉm b√°n, t√†i kho·∫£n b·ªã kh√≥a do nh·∫≠p sai MK nhi·ªÅu l·∫ßn
X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: Quy tr√¨nh Qu√™n/ƒê·ªïi m·∫≠t kh·∫©u
X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i: B∆∞·ªõc 1 - Thu th·∫≠p th√¥ng tin v√† x√°c nh·∫≠n t√†i kho·∫£n
Chu·ªói h√†nh ƒë·ªông: x√°c nh·∫≠n outlet + m√£ ‚Üí xin ·∫£nh l·ªói ‚Üí h∆∞·ªõng d·∫´n Qu√™n MK (nh·∫≠p m√£ outlet ‚Üí OTP ‚Üí t·∫°o MK m·ªõi 12 k√Ω t·ª±, kh√¥ng tr√πng c≈©)

‚úÖ
Nh·∫≠n di·ªán t√¨nh hu·ªëng: KH qu√™n SƒêT ƒëƒÉng k√Ω, kh√¥ng nh·∫≠n ƒë∆∞·ª£c OTP
X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: Quy tr√¨nh Qu√™n/ƒê·ªïi m·∫≠t kh·∫©u - X·ª≠ l√Ω t√¨nh hu·ªëng ƒë·∫∑c bi·ªát
X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i: B∆∞·ªõc 5 - X·ª≠ l√Ω t√¨nh hu·ªëng kh√¥ng nh·∫≠n OTP
Chu·ªói h√†nh ƒë·ªông: y√™u c·∫ßu th√¥ng tin (t√™n CH, m√£ CH, t√™n ng∆∞·ªùi li√™n h·ªá, SƒêT m·ªõi) ‚Üí chuy·ªÉn b·ªô ph·∫≠n c·∫≠p nh·∫≠t

‚úÖ
Nh·∫≠n di·ªán t√¨nh hu·ªëng: KH h·ªèi m√£ NV ƒë·ªÉ c√†i app, c√≥ v·∫ª ƒë√£ ƒëƒÉng k√Ω
X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: Quy tr√¨nh Qu√™n/ƒê·ªïi m·∫≠t kh·∫©u - KH ƒë√£ c√≥ t√†i kho·∫£n
X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i: B∆∞·ªõc 1 - X√°c ƒë·ªãnh t√†i kho·∫£n
Chu·ªói h√†nh ƒë·ªông: h·ªèi t√™n CH + SƒêT ‚Üí tra c·ª©u t√†i kho·∫£n ‚Üí h∆∞·ªõng d·∫´n ƒëƒÉng nh·∫≠p ho·∫∑c Qu√™n MK

‚úÖ
Nh·∫≠n di·ªán t√¨nh hu·ªëng: KH ƒë√£ gi·∫£i quy·∫øt xong v·∫•n ƒë·ªÅ, c·∫£m ∆°n
X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: Quy tr√¨nh Qu√™n/ƒê·ªïi m·∫≠t kh·∫©u
X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i: B∆∞·ªõc 6 - X√°c nh·∫≠n ho√†n t·∫•t v√† cung c·∫•p k√™nh h·ªó tr·ª£
Chu·ªói h√†nh ƒë·ªông: h·ªèi c√≤n c·∫ßn g√¨ kh√¥ng ‚Üí cung c·∫•p k√™nh h·ªó tr·ª£ (Zalo/hotline 1800234522)


QUY T·∫ÆC VI·∫æT:
0. Tu√¢n th·ªß nghi√™m ng·∫∑t quy tr√¨nh ƒë√£ cho
1. Format: Nh·∫≠n di·ªán t√¨nh hu·ªëng: ... X√°c ƒë·ªãnh quy tr√¨nh √°p d·ª•ng: ... X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i trong quy tr√¨nh: ... X√°c ƒë·ªãnh chu·ªói h√†nh ƒë·ªông ti·∫øp theo: action A ‚Üí action B ‚Üí action C ...
2. S·ª≠ d·ª•ng t·ª´ ng·ªØ ng·∫Øn g·ªçn, s√∫c t√≠ch
3. Chi ti·∫øt ph·ª• trong ngo·∫∑c ƒë∆°n ()
"""
        return prompt
    async def process_conversation(self, conversation: Dict) -> Dict:
        """Process entire conversation with memory of previous turns"""
        # Get procedure and metadata
        procedure_id = str(conversation.get('procedure_id'))  # Default to procedure 2 (forget password)
        if not procedure_id or procedure_id not in procedure:
            raise ValueError(f"Invalid or missing procedure_id: {procedure_id}")
        procedure_detail = procedure.get(procedure_id, {}).get('detail_description', 'N/A')
        procedure_name = procedure.get(procedure_id, {}).get('name', 'N/A')
        # Build system prompt once
        system_prompt = self._build_system_prompt(procedure_name, procedure_detail)
        llm = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature
        ).with_structured_output(ThinkingTeacherResponse)
        # Initialize conversation memory
        memory: List[BaseMessage] = [SystemMessage(content=system_prompt)]

        original_messages = conversation.get('messages', [])
        enhanced_messages = []

        # Process each turn
        i = 0
        while i < len(original_messages):
            current_msg = original_messages[i]

            # User message - just add to memory and output
            if current_msg.get('role') == 'user':
                enhanced_messages.append(current_msg)
                memory.append(HumanMessage(content=current_msg.get('content', '')))
                i += 1
                continue

            # Assistant message - need to evaluate and enhance
            if current_msg.get('role') == 'assistant':
                # Get corresponding user message (should be previous message)

                # Format current turn as if YOU (the agent) are handling this turn
                current_turn = f"""
TIN NH·∫ÆN b·∫°n chu·∫©n b·ªã tr·∫£ l·ªùi cho KH:
{current_msg.get('content', '')}
H√£y vi·∫øt thinking (k·∫ø ho·∫°ch h√†nh ƒë·ªông n·ªôi t√¢m) c·ªßa b·∫°n chu·∫©n b·ªã cho TIN NHƒÇN n√†y."""

                # Add to memory and get response
                memory.append(HumanMessage(content=current_turn))
                response = await llm.ainvoke(memory)
                if isinstance(response, dict):
                    response = ThinkingTeacherResponse(**response)

                # Create enhanced message with thinking
                enhanced_msg = {
                    'role': 'assistant',
                    'content': f"<thinking>{response.reasoning}</thinking>\n{current_msg.get('content', '')}",
                }
                enhanced_messages.append(enhanced_msg)

                # Update memory with the thinking and response for next turn
                # drop the last user message (current_turn)
                memory = memory[:-1]
                memory.append(AIMessage(content=enhanced_msg['content']))

                i += 1

        # Return enhanced conversation
        conversation['messages'] = enhanced_messages
        return conversation

    def _format_memory_history(self, messages: List[Dict]) -> str:
        """Format previous messages for context"""
        if not messages:
            return "(Ch∆∞a c√≥ l·ªãch s·ª≠)"

        history_lines = []
        for msg in messages:
            role = msg.get('role')
            content = msg.get('content', '')
            if role == 'user':
                history_lines.append(f"KH: {content}")
            elif role == 'assistant':
                # Show the final response (could be corrected or original)
                history_lines.append(f"Agent: {content}")

        return "\n".join(history_lines)
    


async def main():
    """Main function with semaphore for concurrent processing"""
    # Semaphore to limit concurrent API calls
    semaphore = asyncio.Semaphore(5)  # Adjust this number based on your API rate limits

    async def process_with_semaphore(conv: Dict, index: int) -> Dict:
        """Process conversation with semaphore control"""
        async with semaphore:
            print(f"Processing conversation {index + 1}/{len(conversations)}...")
            teacher = ThinkingTeacher(model_name="gpt-4.1")
            try:
                result = await teacher.process_conversation(conv)
                print(f"‚úì Completed conversation {index + 1}")
                return result
            except Exception as e:
                print(f"‚úó Error in conversation {index + 1}: {str(e)}")
                # Return original conversation if error occurs
                return conv

    # Process all conversations concurrently with semaphore
    tasks = [process_with_semaphore(conv, i) for i, conv in enumerate(conversations)]
    thinking_conversations = await asyncio.gather(*tasks)

    # Save results
    output_path = "/home/namnp/ChatBotSynthetic/data/thinking_teacher_conversations.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(thinking_conversations, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Successfully processed {len(thinking_conversations)} conversations")
    print(f"üìù Saved to: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())